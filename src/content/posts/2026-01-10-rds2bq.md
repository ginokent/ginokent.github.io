---
title: "RDS Aurora のデータを BigQuery に日次転送するパイプラインを Terraform で構築する"
description: "AWS RDS Aurora のデータを GCP BigQuery に日次で転送するパイプラインを構築する機会があったので、構成と実装を書き残す。インターネット上には CDC を使うだの高カロリーな実装が多かったので、 RDS Snapshot の S3 出力をベースに RDS に余計な設定を追加したりせずに済み、かつコスト安な実装を心がけた。"
publishedAt: 2026-01-10
updatedAt: 2026-01-12
tags: ["AWS", "GCP", "RDS", "BigQuery", "Terraform", "Step Functions", "Storage Transfer Service", "Cloud Functions"]
---

AWS RDS Aurora のデータを GCP BigQuery に日次で転送するパイプラインを構築する機会があったので、構成と実装を書き残す。
インターネット上には CDC を使うだの高カロリーな実装が多かったので、 RDS Snapshot の S3 出力をベースに RDS に余計な設定を追加したりせずに済み、かつコスト安な実装を心がけた。

## 概要

- RDS の Export to S3 機能で Parquet 形式で S3 にエクスポート
- Storage Transfer Service で S3 → GCS に転送
- Cloud Functions でパス構造を BigQuery 外部テーブル向けに再編成
- BigQuery 外部テーブルで GCS 上の Parquet を参照
- 全体を Terraform で IaC 化

## 全体構成

```
+---------------------------------------------------------------------------------------------+
|                                          AWS                                                |
|                                                                                             |
|  +---------------------+     +-----------------------------------------------------------+  |
|  | EventBridge         |     | Step Functions (rds2bq-rds2s3)                            |  |
|  | Scheduler           |     |                                                           |  |
|  |                     |     |  +-----------+   +------------+   +-----------------+     |  |
|  | Trigger at      ------------>|BuildPrefix|-->|CreateSnap- |-->|DescribeSnapshot |     |  |
|  | JST 00:30           |     |  |(parse dt) |   |shot        |   |(poll until done)|     |  |
|  +---------------------+     |  +-----------+   +------------+   +--------+--------+     |  |
|                              |                                           |               |  |
|                              |                                           v               |  |
|                              |                                  +-----------------+      |  |
|                              |                                  | StartExportTask |      |  |
|                              |                                  | (start S3 export)|     |  |
|                              |                                  +--------+--------+      |  |
|                              +-------------------------------------------+---------------+  |
|                                                                          |                  |
|  +---------------------+                                                 |                  |
|  | RDS Aurora          |                                                 v                  |
|  |                     |     +------------------------------------------------------+       |
|  |  +---------------+  |     | RDS Export Task (async, 15-30 min)                   |       |
|  |  |   Database    |------->|  Snapshot -> Parquet conversion -> S3 output         |       |
|  |  +---------------+  |     |  (KMS encrypted)                                     |       |
|  +---------------------+     +----------------------------------+-------------------+       |
|                                                                 |                           |
|                                                                 v                           |
|                              +------------------------------------------------------+       |
|                              | S3 Bucket                                            |       |
|                              |  exports/year=YYYY/month=MM/date=DD/.../*.parquet    |       |
|                              +----------------------------------+-------------------+       |
|                                                                 |                           |
+---------------------------------------------------------------------------------------------+
                                                                  |
                                                                  | Storage Transfer Service
                                                                  | (hourly)
                                                                  v
+---------------------------------------------------------------------------------------------+
|                                          GCP                                                |
|                                                                                             |
|                              +------------------------------------------------------+       |
|                              | GCS Bucket                                           |       |
|                              |  exports/        (raw data copied from S3)           |       |
|                              |  reorganized/    (reorganized for BigQuery)          |       |
|                              +----------------------------------+-------------------+       |
|                                                                 |                           |
|                                       Eventarc (object.finalized)                           |
|                                                                 v                           |
|                              +------------------------------------------------------+       |
|                              | Cloud Functions (rds2bq-reorganizer)                 |       |
|                              |  Copy: exports/ -> reorganized/                      |       |
|                              +----------------------------------+-------------------+       |
|                                                                 |                           |
|                                                                 v                           |
|                              +------------------------------------------------------+       |
|                              | BigQuery (External Tables)                           |       |
|                              |  -> references reorganized/.../*.parquet             |       |
|                              +------------------------------------------------------+       |
|                                                                                             |
+---------------------------------------------------------------------------------------------+
```

## 処理時間の目安

手元の検証環境での計測結果 (RDS Snapshot 1 GiB の場合):

| 処理 | 所要時間 |
|------|---------|
| RDS Snapshot 作成 | 約 3 分 |
| S3 Export Task 開始待ち | 約 15 分 |
| S3 Export Task 実行 | 約 15 分 |
| S3 → GCS 転送 | 約 30 秒 |
| **合計** | **約 30-35 分** |

## 主要コンポーネント

| コンポーネント | 役割 |
|--------------|------|
| AWS KMS | RDS Export 時の暗号化キー |
| S3 Bucket | Export 先 (TLS/KMS 必須, 30 日で自動削除) |
| Step Functions | スナップショット作成 → Export のオーケストレーション |
| EventBridge Scheduler | 日次スケジューラー (JST 00:30) |
| Storage Transfer Service | S3 → GCS の定期転送 (1 時間ごと) |
| Cloud Functions | Parquet ファイルのパス再編成 |
| BigQuery | 外部テーブルとして Parquet を参照 |

---

## 前提条件

- AWS と GCP の両方に Terraform でリソースを作成できる権限があること
- RDS Aurora クラスターが既に存在すること
- Cloud Functions のソースコード (後述) を `./functions/rds2bq-reorganizer/` に配置すること

## ディレクトリ構成

```
.
├── main.tf          # この記事の Terraform コード
└── functions/
    └── rds2bq-reorganizer/
        ├── main.go  # Cloud Functions のソースコード
        └── go.mod
```

---

## 変数定義

```hcl main.tf
variable "gcp_project_id" {
  description = "GCP プロジェクト ID"
  type        = string
}

variable "aws_region" {
  description = "AWS リージョン"
  type        = string
  default     = "ap-northeast-1"
}

variable "gcp_region" {
  description = "GCP リージョン"
  type        = string
  default     = "asia-northeast1"
}

variable "rds_cluster_identifier" {
  description = "RDS Aurora クラスターの識別子"
  type        = string
}

variable "database_name" {
  description = "転送対象のデータベース名"
  type        = string
}

variable "table_names" {
  description = "転送対象のテーブル名リスト"
  type        = list(string)
}
```

`terraform.tfvars` の例:

```hcl terraform.tfvars
gcp_project_id         = "my-gcp-project"
aws_region             = "ap-northeast-1"
gcp_region             = "asia-northeast1"
rds_cluster_identifier = "my-aurora-cluster"
database_name          = "mydb"
table_names = [
  "users",
  "orders",
  "products",
]
```

## データソース

```hcl main.tf
data "aws_caller_identity" "current" {}

data "google_project" "current" {
  project_id = var.gcp_project_id
}

data "google_storage_transfer_project_service_account" "current" {
  project = data.google_project.current.name
}

locals {
  rds2bq_export_path_prefix = "exports"
}
```

Storage Transfer Service を使用する前に、以下のコマンドでサービスアカウントを有効化しておく:

```bash
gcloud transfer authorize --add-missing --project=${GCP_PROJECT_ID}
```

---

## AWS 側の実装

### KMS キーの作成

RDS の Export to S3 機能は KMS による暗号化が必須。

```hcl main.tf
resource "aws_kms_key" "rds2bq" {
  description         = "KMS for rds2bq"
  enable_key_rotation = true
}

resource "aws_kms_alias" "rds2bq" {
  name          = "alias/rds2bq"
  target_key_id = aws_kms_key.rds2bq.key_id
}

resource "aws_kms_key_policy" "rds2bq" {
  key_id = aws_kms_key.rds2bq.key_id
  policy = data.aws_iam_policy_document.rds2bq_kms_key_policy.json
}

data "aws_iam_policy_document" "rds2bq_kms_key_policy" {
  # Administrator 権限
  statement {
    sid    = "AllowRootAccountAdmin"
    effect = "Allow"
    principals {
      type        = "AWS"
      identifiers = ["arn:aws:iam::${data.aws_caller_identity.current.account_id}:root"]
    }
    actions   = ["kms:*"]
    resources = ["*"]
  }

  # RDS Export サービスに CreateGrant を許可
  statement {
    sid    = "AllowRdsExportCreateGrant"
    effect = "Allow"
    principals {
      type        = "Service"
      identifiers = ["export.rds.amazonaws.com"]
    }
    actions   = ["kms:CreateGrant", "kms:DescribeKey"]
    resources = ["*"]
  }

  # Scheduler ロールに DescribeKey を許可
  statement {
    sid    = "AllowDescribeKeyToSchedulerRole"
    effect = "Allow"
    principals {
      type        = "AWS"
      identifiers = [aws_iam_role.rds2bq_rds2s3_scheduler.arn]
    }
    actions   = ["kms:DescribeKey"]
    resources = ["*"]
  }

  # S3→GCS 転送ロールに Decrypt を許可
  statement {
    sid    = "AllowDecryptToImportRole"
    effect = "Allow"
    principals {
      type        = "AWS"
      identifiers = [aws_iam_role.rds2bq_s32gcs.arn]
    }
    actions   = ["kms:Decrypt", "kms:DescribeKey"]
    resources = ["*"]
  }
}
```

### S3 バケットの作成

Export 先の S3 バケット。セキュリティのため以下を設定:

- パブリックアクセスを完全にブロック
- TLS 必須
- 指定 KMS キーでの暗号化必須
- Export ロールからの書き込みのみ許可
- 30 日で自動削除

```hcl main.tf
resource "aws_s3_bucket" "rds2bq" {
  bucket        = "rds2bq-${data.aws_caller_identity.current.account_id}-${var.aws_region}"
  force_destroy = true
}

resource "aws_s3_bucket_public_access_block" "rds2bq" {
  bucket                  = aws_s3_bucket.rds2bq.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

resource "aws_s3_bucket_lifecycle_configuration" "rds2bq" {
  bucket = aws_s3_bucket.rds2bq.id
  rule {
    id     = "expire-30d"
    status = "Enabled"
    expiration { days = 30 }
  }
}

resource "aws_s3_bucket_policy" "rds2bq" {
  bucket = aws_s3_bucket.rds2bq.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Sid       = "DenyNonTLS"
        Effect    = "Deny"
        Principal = "*"
        Action    = "s3:*"
        Resource  = [aws_s3_bucket.rds2bq.arn, "${aws_s3_bucket.rds2bq.arn}/*"]
        Condition = { Bool = { "aws:SecureTransport" = "false" } }
      },
      {
        Sid       = "DenyPutWithoutSSEKMS"
        Effect    = "Deny"
        Principal = "*"
        Action    = "s3:PutObject"
        Resource  = "${aws_s3_bucket.rds2bq.arn}/${local.rds2bq_export_path_prefix}/*"
        Condition = { StringNotEquals = { "s3:x-amz-server-side-encryption" = "aws:kms" } }
      },
      {
        Sid       = "DenyPutWithWrongKmsKey"
        Effect    = "Deny"
        Principal = "*"
        Action    = "s3:PutObject"
        Resource  = "${aws_s3_bucket.rds2bq.arn}/${local.rds2bq_export_path_prefix}/*"
        Condition = {
          StringNotEquals = {
            "s3:x-amz-server-side-encryption-aws-kms-key-id" = [
              aws_kms_key.rds2bq.arn,
              aws_kms_key.rds2bq.key_id,
              aws_kms_alias.rds2bq.arn
            ]
          }
        }
      },
      {
        Sid       = "DenyPutIfNotExportRole"
        Effect    = "Deny"
        Principal = "*"
        Action    = ["s3:PutObject"]
        Resource  = "${aws_s3_bucket.rds2bq.arn}/${local.rds2bq_export_path_prefix}/*"
        Condition = {
          StringNotEquals = {
            "aws:PrincipalArn" = aws_iam_role.rds2bq_rds2s3_rdsexport.arn
          }
        }
      }
    ]
  })
}
```

### RDS Export 用 IAM ロール

`export.rds.amazonaws.com` が AssumeRole できるロールを作成:

```hcl main.tf
resource "aws_iam_role" "rds2bq_rds2s3_rdsexport" {
  name = "rds2bq-rds2s3-rdsexport"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Sid       = "OnlyRdsExportServiceInThisAccount",
      Effect    = "Allow",
      Principal = { Service = "export.rds.amazonaws.com" },
      Action    = "sts:AssumeRole",
      Condition = { StringEquals = { "aws:SourceAccount" = data.aws_caller_identity.current.account_id } }
    }]
  })
}

resource "aws_iam_policy" "rds2bq_rds2s3_rdsexport" {
  name = "rds2bq-rds2s3-rdsexport"
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Sid      = "ObjectRWUnderExportsPrefix",
        Effect   = "Allow",
        Action   = ["s3:PutObject*", "s3:GetObject*", "s3:DeleteObject*", "s3:AbortMultipartUpload", "s3:ListMultipartUploadParts"],
        Resource = "${aws_s3_bucket.rds2bq.arn}/${local.rds2bq_export_path_prefix}/*"
      },
      {
        Sid      = "ListBucketWithPrefix",
        Effect   = "Allow",
        Action   = ["s3:ListBucket", "s3:GetBucketLocation", "s3:ListBucketMultipartUploads"],
        Resource = aws_s3_bucket.rds2bq.arn
      },
      {
        Sid      = "UseKmsKeyForSnapshotExport",
        Effect   = "Allow",
        Action   = ["kms:Encrypt", "kms:Decrypt", "kms:ReEncrypt*", "kms:GenerateDataKey*", "kms:DescribeKey"],
        Resource = aws_kms_key.rds2bq.arn
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "rds2bq_rds2s3_rdsexport" {
  role       = aws_iam_role.rds2bq_rds2s3_rdsexport.name
  policy_arn = aws_iam_policy.rds2bq_rds2s3_rdsexport.arn
}
```

### Step Functions

RDS Export to S3 は非同期処理なので、Step Functions でオーケストレーションする。

フローは以下の通り:

1. `BuildPrefix`: スケジュール時刻から日付パーティションのパスを生成
2. `CreateSnapshot`: DB クラスタースナップショットを作成
3. `DescribeSnapshot`: スナップショットのステータスをポーリング
4. `StartExport`: S3 Export Task を開始

```hcl main.tf
resource "aws_iam_role" "rds2bq_rds2s3" {
  name = "rds2bq-rds2s3"
  assume_role_policy = jsonencode({
    Version   = "2012-10-17",
    Statement = [{ Effect = "Allow", Principal = { Service = "states.amazonaws.com" }, Action = "sts:AssumeRole" }]
  })
}

resource "aws_iam_policy" "rds2bq_rds2s3" {
  name = "rds2bq-rds2s3"
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      { Sid = "StartExport", Effect = "Allow", Action = ["rds:StartExportTask"], Resource = "*" },
      { Sid = "CreateSnapshot", Effect = "Allow", Action = ["rds:CreateDBClusterSnapshot"], Resource = "*" },
      { Sid = "DescribeSnapshot", Effect = "Allow", Action = ["rds:DescribeDBClusterSnapshots"], Resource = "*" },
      { Sid = "TagSnapshot", Effect = "Allow", Action = ["rds:AddTagsToResource", "rds:ListTagsForResource", "rds:RemoveTagsFromResource"], Resource = "*" },
      {
        Sid       = "PassExportRole",
        Effect    = "Allow",
        Action    = "iam:PassRole",
        Resource  = aws_iam_role.rds2bq_rds2s3_rdsexport.arn,
        Condition = { StringEquals = { "iam:PassedToService" = "rds.amazonaws.com" } }
      },
      { Sid = "AllowKmsCreateGrantForExport", Effect = "Allow", Action = ["kms:CreateGrant", "kms:DescribeKey"], Resource = aws_kms_key.rds2bq.arn },
      { Sid = "AllowCloudWatchLogsDelivery", Effect = "Allow", Action = ["logs:CreateLogDelivery", "logs:GetLogDelivery", "logs:UpdateLogDelivery", "logs:DeleteLogDelivery", "logs:ListLogDeliveries", "logs:PutResourcePolicy", "logs:DescribeResourcePolicies", "logs:DescribeLogGroups"], Resource = "*" }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "rds2bq_rds2s3" {
  role       = aws_iam_role.rds2bq_rds2s3.name
  policy_arn = aws_iam_policy.rds2bq_rds2s3.arn
}

resource "aws_cloudwatch_log_group" "rds2bq_rds2s3" {
  name              = "/aws/states/rds2bq-rds2s3"
  retention_in_days = 30
}

resource "aws_sfn_state_machine" "rds2bq_rds2s3" {
  name     = "rds2bq-rds2s3"
  role_arn = aws_iam_role.rds2bq_rds2s3.arn
  definition = jsonencode({
    Comment = "Build date partition prefix, create/await DB cluster snapshot, then StartExportTask",
    StartAt = "BuildPrefix",
    States = {
      # $.scheduled_time は ISO8601 (例: 2026-01-08T18:42:21Z) 前提
      BuildPrefix = {
        Type = "Pass",
        Parameters = {
          "year.$"   = "States.ArrayGetItem(States.StringSplit($.scheduled_time, '-'), 0)",
          "month.$"  = "States.ArrayGetItem(States.StringSplit($.scheduled_time, '-'), 1)",
          "day.$"    = "States.ArrayGetItem(States.StringSplit(States.ArrayGetItem(States.StringSplit($.scheduled_time, 'T'), 0), '-'), 2)",
          "hour.$"   = "States.ArrayGetItem(States.StringSplit(States.ArrayGetItem(States.StringSplit($.scheduled_time, 'T'), 1), ':'), 0)",
          "minute.$" = "States.ArrayGetItem(States.StringSplit(States.ArrayGetItem(States.StringSplit($.scheduled_time, 'T'), 1), ':'), 1)",
          "second.$" = "States.ArrayGetItem(States.StringSplit(States.ArrayGetItem(States.StringSplit(States.ArrayGetItem(States.StringSplit($.scheduled_time, 'T'), 1), ':'), 2), '.Zz+-'), 0)"
        },
        ResultPath = "$.built",
        Next       = "CreateSnapshot"
      },

      CreateSnapshot = {
        Type     = "Task",
        Resource = "arn:aws:states:::aws-sdk:rds:createDBClusterSnapshot",
        Parameters = {
          "DbClusterIdentifier"           = var.rds_cluster_identifier,
          "DbClusterSnapshotIdentifier.$" = "States.Format('rds2bq-{}{}{}t{}{}{}', $.built.year, $.built.month, $.built.day, $.built.hour, $.built.minute, $.built.second)"
        },
        ResultPath = "$.create",
        Catch = [{
          ErrorEquals = ["States.ALL"],
          ResultPath  = "$.create_error",
          Next        = "WaitBeforeDescribe"
        }],
        Next = "WaitBeforeDescribe"
      },

      WaitBeforeDescribe = { Type = "Wait", Seconds = 20, Next = "DescribeSnapshot" },

      DescribeSnapshot = {
        Type     = "Task",
        Resource = "arn:aws:states:::aws-sdk:rds:describeDBClusterSnapshots",
        Parameters = {
          "DbClusterSnapshotIdentifier.$" = "States.Format('rds2bq-{}{}{}t{}{}{}', $.built.year, $.built.month, $.built.day, $.built.hour, $.built.minute, $.built.second)"
        },
        ResultPath = "$.desc",
        Catch = [{
          ErrorEquals = ["States.ALL"],
          ResultPath  = "$.describe_error",
          Next        = "WaitAndRetryDescribe"
        }],
        Next = "CheckSnapshotStatus"
      },

      CheckSnapshotStatus = {
        Type = "Choice",
        Choices = [
          { Variable = "$.desc.DbClusterSnapshots[0].Status", StringEquals = "available", Next = "StartExport" },
          { Variable = "$.desc.DbClusterSnapshots[0].Status", StringEquals = "failed", Next = "FailSnapshot" }
        ],
        Default = "WaitAndRetryDescribe"
      },

      WaitAndRetryDescribe = { Type = "Wait", Seconds = 15, Next = "DescribeSnapshot" },

      StartExport = {
        Type     = "Task",
        Resource = "arn:aws:states:::aws-sdk:rds:startExportTask",
        Parameters = {
          "ExportTaskIdentifier.$" = "States.Format('rds2bq-{}{}{}t{}{}{}', $.built.year, $.built.month, $.built.day, $.built.hour, $.built.minute, $.built.second)",
          "SourceArn.$"            = "$.desc.DbClusterSnapshots[0].DbClusterSnapshotArn",
          "S3BucketName"           = aws_s3_bucket.rds2bq.bucket,
          "S3Prefix.$"             = "States.Format('${local.rds2bq_export_path_prefix}/year={}/month={}/date={}', $.built.year, $.built.month, $.built.day)",
          "IamRoleArn"             = aws_iam_role.rds2bq_rds2s3_rdsexport.arn,
          "KmsKeyId"               = aws_kms_key.rds2bq.arn
        },
        End = true
      },

      FailSnapshot = { Type = "Fail", Error = "SnapshotFailed", Cause = "DB cluster snapshot status=failed" }
    }
  })
  logging_configuration {
    include_execution_data = true
    level                  = "ALL"
    log_destination        = "${aws_cloudwatch_log_group.rds2bq_rds2s3.arn}:*"
  }
}
```

`BuildPrefix` の `second.$` が複雑なのは、ISO8601 の末尾にタイムゾーン情報 (`Z` や `+09:00`) が含まれるため。単純に `:` でスプリットすると末尾に余計な文字が混ざる。

### EventBridge Scheduler

JST 00:30 に Step Functions を起動するスケジューラー:

```hcl main.tf
resource "aws_iam_role" "rds2bq_rds2s3_scheduler" {
  name = "rds2bq-rds2s3-scheduler"
  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect    = "Allow",
      Principal = { Service = "scheduler.amazonaws.com" },
      Action    = "sts:AssumeRole"
    }]
  })
}

resource "aws_iam_policy" "rds2bq_rds2s3_scheduler" {
  name = "rds2bq-rds2s3-scheduler"
  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      { Sid = "AllowStartExportTask", Effect = "Allow", Action = ["rds:StartExportTask"], Resource = "*" },
      { Sid = "AllowStartStepFn", Effect = "Allow", Action = ["states:StartExecution"], Resource = aws_sfn_state_machine.rds2bq_rds2s3.arn },
      {
        Sid      = "AllowPassOnlyExportRoleToRds",
        Effect   = "Allow",
        Action   = "iam:PassRole",
        Resource = aws_iam_role.rds2bq_rds2s3_rdsexport.arn,
        Condition = { StringEquals = { "iam:PassedToService" = "rds.amazonaws.com" } }
      }
    ]
  })
}

resource "aws_iam_role_policy_attachment" "rds2bq_rds2s3_scheduler" {
  role       = aws_iam_role.rds2bq_rds2s3_scheduler.name
  policy_arn = aws_iam_policy.rds2bq_rds2s3_scheduler.arn
}

resource "aws_scheduler_schedule" "rds2bq_rds2s3" {
  name                = "rds2bq-rds2s3"
  group_name          = "default"
  description         = "Daily: Step Functions -> RDS StartExportTask"
  schedule_expression = "cron(30 15 * * ? *)" # UTC 15:30 = JST 00:30
  flexible_time_window { mode = "OFF" }

  target {
    arn      = "arn:aws:scheduler:::aws-sdk:sfn:startExecution"
    role_arn = aws_iam_role.rds2bq_rds2s3_scheduler.arn
    # NOTE: jsonencode を使うと <aws.scheduler.scheduled-time> がエスケープされるためヒアドキュメントを使用
    input = <<-EOF
    {
      "StateMachineArn": "${aws_sfn_state_machine.rds2bq_rds2s3.arn}",
      "Input": "{\"scheduled_time\":\"<aws.scheduler.scheduled-time>\"}"
    }
    EOF
  }
}
```

`jsonencode` を使うと `<aws.scheduler.scheduled-time>` が `\u003c...\u003e` にエスケープされ、EventBridge Scheduler がコンテキスト変数として認識できなくなる。ヒアドキュメントを使う必要がある。

### S3 → GCS 転送用 IAM ロール

Storage Transfer Service が AWS の S3 にアクセスするための IAM ロール。Web Identity Federation で GCP のサービスアカウントを信頼する:

```hcl main.tf
resource "aws_iam_role" "rds2bq_s32gcs" {
  name = "rds2bq-s32gcs"
  assume_role_policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [{
      "Effect" : "Allow",
      "Principal" : { "Federated" : "accounts.google.com" },
      "Action" : "sts:AssumeRoleWithWebIdentity",
      "Condition" : {
        "StringEquals" : {
          "accounts.google.com:sub" : data.google_storage_transfer_project_service_account.current.subject_id
        }
      }
    }]
  })
}

resource "aws_iam_policy" "rds2bq_s32gcs" {
  name = "rds2bq-s32gcs"
  policy = jsonencode({
    "Version" : "2012-10-17",
    "Statement" : [{
      "Effect" : "Allow",
      "Action" : ["s3:Get*", "s3:List*", "s3:Delete*"],
      "Resource" : "*"
    }]
  })
}

resource "aws_iam_role_policy_attachment" "rds2bq_s32gcs" {
  role       = aws_iam_role.rds2bq_s32gcs.name
  policy_arn = aws_iam_policy.rds2bq_s32gcs.arn
}
```

---

## GCP 側の実装

### Storage Transfer Service の権限設定

```hcl main.tf
resource "google_project_iam_member" "storage_transfer_storage_admin" {
  project = data.google_project.current.name
  role    = "roles/storage.admin"
  member  = data.google_storage_transfer_project_service_account.current.member
}
```

### GCS バケットと Storage Transfer Job

```hcl main.tf
resource "google_storage_bucket" "rds2bq" {
  name                        = "${data.google_project.current.name}-rds2bq"
  location                    = var.gcp_region
  uniform_bucket_level_access = true
  public_access_prevention    = "enforced"
  lifecycle_rule {
    action { type = "Delete" }
    condition { age = 30 }
  }
}

resource "google_storage_bucket_iam_member" "rds2bq_storage_transfer" {
  bucket = google_storage_bucket.rds2bq.name
  role   = "roles/storage.objectAdmin"
  member = data.google_storage_transfer_project_service_account.current.member
}

resource "google_storage_transfer_job" "rds2bq_s32gcs_daily" {
  description = "Daily copy Parquet from AWS S3 to GCS"
  project     = data.google_project.current.name
  transfer_spec {
    aws_s3_data_source {
      bucket_name             = aws_s3_bucket.rds2bq.bucket
      managed_private_network = true
      role_arn                = aws_iam_role.rds2bq_s32gcs.arn
    }
    gcs_data_sink {
      bucket_name = google_storage_bucket.rds2bq.name
    }
    object_conditions {
      include_prefixes = ["${local.rds2bq_export_path_prefix}/"]
    }
    transfer_options {
      overwrite_objects_already_existing_in_sink = false
    }
  }
  schedule {
    schedule_start_date {
      year  = 2025
      month = 1
      day   = 1
    }
    repeat_interval = "3600s" # 1時間ごと
  }
}
```

### Cloud Functions (パス再編成)

RDS Export to S3 の出力パス構造は以下の通り:

```
exports/year=2026/month=01/date=09/rds2bq-20260109t003000/{database}/{database}.{table}/{partition}/part-00000.parquet
```

BigQuery 外部テーブルで参照しやすいように、以下の構造に変換する:

```
reorganized/dbcluster/{cluster_id}/database/{database}/table/{table}/part-00000.parquet
```

#### Cloud Functions のソースコード

`./functions/rds2bq-reorganizer/main.go`:

```go main.go
package rds2bqreorganizer

import (
	"context"
	"errors"
	"fmt"
	"os"
	"path/filepath"
	"regexp"
	"strings"

	"cloud.google.com/go/storage"
	"github.com/GoogleCloudPlatform/functions-framework-go/functions"
	"github.com/cloudevents/sdk-go/v2/event"
)

func init() {
	functions.CloudEvent("ReorganizeParquet", reorganizeParquet)
}

type StorageObjectData struct {
	Bucket string `json:"bucket"`
	Name   string `json:"name"`
}

func reorganizeParquet(ctx context.Context, e event.Event) error {
	var data StorageObjectData
	if err := e.DataAs(&data); err != nil {
		return fmt.Errorf("event.DataAs: %w", err)
	}

	// exports/ 配下の .parquet ファイルのみ処理
	if !strings.HasPrefix(data.Name, "exports/") || !strings.HasSuffix(data.Name, ".parquet") {
		return nil
	}

	clusterID := os.Getenv("DB_CLUSTER_IDENTIFIER")
	if clusterID == "" {
		return errors.New("DB_CLUSTER_IDENTIFIER environment variable is not set")
	}

	newPath, err := buildNewPath(data.Name, clusterID)
	if err != nil {
		return nil // パースできないファイルはスキップ
	}

	client, err := storage.NewClient(ctx)
	if err != nil {
		return fmt.Errorf("storage.NewClient: %w", err)
	}
	defer client.Close()

	src := client.Bucket(data.Bucket).Object(data.Name)
	dst := client.Bucket(data.Bucket).Object(newPath)

	if _, err := dst.CopierFrom(src).Run(ctx); err != nil {
		return fmt.Errorf("copy failed: %w", err)
	}

	return nil
}

var pathPattern = regexp.MustCompile(
	`^exports/year=(\d{4})/month=(\d{2})/date=(\d{2})/([^/]+)/([^/]+)/([^/]+)/([^/]+)/(.+\.parquet)$`,
)

func buildNewPath(srcPath, clusterID string) (string, error) {
	matches := pathPattern.FindStringSubmatch(srcPath)
	if matches == nil {
		return "", fmt.Errorf("path does not match expected pattern: %s", srcPath)
	}

	database := matches[5]
	dbTable := matches[6] // database.table 形式
	filename := matches[8]

	// database.table から table 名を抽出
	table := strings.TrimPrefix(dbTable, database+".")

	return filepath.Join(
		"reorganized",
		"dbcluster", clusterID,
		"database", database,
		"table", table,
		filename,
	), nil
}
```

`./functions/rds2bq-reorganizer/go.mod`:

```
module rds2bqreorganizer

go 1.24

require (
	cloud.google.com/go/storage v1.43.0
	github.com/GoogleCloudPlatform/functions-framework-go v1.8.1
	github.com/cloudevents/sdk-go/v2 v2.15.2
)
```

#### Terraform での Cloud Functions デプロイ

```hcl main.tf
# Cloud Functions ソースコード用バケット
resource "google_storage_bucket" "rds2bq_functions" {
  name                        = "${data.google_project.current.name}-rds2bq-functions"
  location                    = var.gcp_region
  uniform_bucket_level_access = true
  public_access_prevention    = "enforced"
}

# ソースコードを ZIP 化
data "archive_file" "rds2bq_reorganizer" {
  type        = "zip"
  source_dir  = "${path.module}/functions/rds2bq-reorganizer"
  output_path = "${path.module}/functions/rds2bq-reorganizer.zip"
}

resource "google_storage_bucket_object" "rds2bq_reorganizer" {
  name   = "rds2bq-reorganizer-${data.archive_file.rds2bq_reorganizer.output_md5}.zip"
  bucket = google_storage_bucket.rds2bq_functions.name
  source = data.archive_file.rds2bq_reorganizer.output_path
}

# Cloud Functions 実行用サービスアカウント
resource "google_service_account" "rds2bq_reorganizer" {
  account_id   = "rds2bq-reorganizer"
  display_name = "Cloud Function rds2bq-reorganizer"
}

resource "google_storage_bucket_iam_member" "rds2bq_reorganizer_object_admin" {
  bucket = google_storage_bucket.rds2bq.name
  role   = "roles/storage.objectAdmin"
  member = google_service_account.rds2bq_reorganizer.member
}

resource "google_project_iam_member" "rds2bq_reorganizer_event_receiver" {
  project = data.google_project.current.project_id
  role    = "roles/eventarc.eventReceiver"
  member  = google_service_account.rds2bq_reorganizer.member
}

# Cloud Functions (2nd gen)
resource "google_cloudfunctions2_function" "rds2bq_reorganizer" {
  name     = "rds2bq-reorganizer"
  location = var.gcp_region
  build_config {
    runtime     = "go124"
    entry_point = "ReorganizeParquet"
    source {
      storage_source {
        bucket = google_storage_bucket.rds2bq_functions.name
        object = google_storage_bucket_object.rds2bq_reorganizer.name
      }
    }
  }
  service_config {
    max_instance_count    = 10
    available_memory      = "256Mi"
    timeout_seconds       = 60
    service_account_email = google_service_account.rds2bq_reorganizer.email
    environment_variables = {
      DB_CLUSTER_IDENTIFIER = var.rds_cluster_identifier
    }
  }
  event_trigger {
    trigger_region        = var.gcp_region
    event_type            = "google.cloud.storage.object.v1.finalized"
    service_account_email = google_service_account.rds2bq_reorganizer.email
    event_filters {
      attribute = "bucket"
      value     = google_storage_bucket.rds2bq.name
    }
    retry_policy = "RETRY_POLICY_RETRY"
  }
}

resource "google_cloud_run_service_iam_member" "rds2bq_reorganizer_invoker" {
  location = google_cloudfunctions2_function.rds2bq_reorganizer.location
  service  = google_cloudfunctions2_function.rds2bq_reorganizer.name
  role     = "roles/run.invoker"
  member   = google_service_account.rds2bq_reorganizer.member
}
```

### BigQuery 外部テーブル

再編成後のパスを参照する外部テーブルを作成:

```hcl main.tf
resource "google_bigquery_dataset" "rds2bq" {
  dataset_id                 = "rds2bq_${var.database_name}"
  location                   = var.gcp_region
  delete_contents_on_destroy = false
}

resource "google_bigquery_table" "rds2bq" {
  for_each            = toset(var.table_names)
  dataset_id          = google_bigquery_dataset.rds2bq.dataset_id
  table_id            = each.key
  deletion_protection = false
  external_data_configuration {
    source_format = "PARQUET"
    autodetect    = true
    source_uris = [
      "gs://${google_storage_bucket.rds2bq.name}/reorganized/dbcluster/${var.rds_cluster_identifier}/database/${var.database_name}/table/${each.key}/*.parquet"
    ]
  }
}
```

`for_each` でテーブル名のリストからまとめて外部テーブルを作成できる。

---

## その他メモ

- RDS Export to S3 は非同期で完了通知がないため、Step Functions でスナップショット完了をポーリングしてから Export を開始している
- Storage Transfer Service は 1 時間ごとに実行。Export 完了タイミングは日によって多少前後するため余裕を持たせている
- Cloud Functions は Eventarc の `object.finalized` トリガーで Parquet ファイルが作成されるたびに起動
- BigQuery 外部テーブルは毎回 GCS を読みに行くため、頻繁にクエリする場合はネイティブテーブルにロードした方が良いかもしれない
- S3 は 30 日、GCS は 30 日でライフサイクルポリシーにより自動削除される
